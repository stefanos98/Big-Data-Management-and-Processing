{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkBwfXmz5I8FuyqP7yMsYF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanos98/Big-Data-Management-and-Processing/blob/main/SentimentAnalysis_Tesla_Twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "eJknEaI--yFp",
        "outputId": "2ffcfef4-d391-46d6-9fa1-4af9d6697078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.11/dist-packages (4.10.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Spark Session Created Successfully!\n",
            "35.197.66.170"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport os\\nos.environ[\\'JAVA_HOME\\'] = \\'C:\\\\Program Files\\\\Java\\\\jdk-17\\'  # Path to JDK\\nos.environ[\\'SPARK_HOME\\'] = \\'C:\\\\spark\\'  # Path to Spark\\nos.environ[\\'PATH\\'] += f\";{os.environ[\\'SPARK_HOME\\']}\\\\bin\"\\n\\n\\nfindspark.init(\\'C:\\\\spark\\')\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from pyspark.sql import SparkSession\n",
        "!pip install pymongo\n",
        "from pymongo import MongoClient\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "!pip install findspark\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TwitterAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session Created Successfully!\")\n",
        "spark.stop()\n",
        "!curl ifconfig.me\n",
        "\n",
        "\"\"\"\n",
        "import os\n",
        "os.environ['JAVA_HOME'] = 'C:\\\\Program Files\\\\Java\\\\jdk-17'  # Path to JDK\n",
        "os.environ['SPARK_HOME'] = 'C:\\spark'  # Path to Spark\n",
        "os.environ['PATH'] += f\";{os.environ['SPARK_HOME']}\\\\bin\"\n",
        "\n",
        "\n",
        "findspark.init('C:\\spark')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo\n",
        "from pymongo import MongoClient\n",
        "import urllib.parse\n",
        "!pip install --upgrade certifi\n",
        "import certifi\n",
        "import ssl\n",
        "\n",
        "# Escape the password using urllib.parse.quote_plus\n",
        "username = \"Stef\"\n",
        "password = \"Stef1!\"\n",
        "escaped_password = urllib.parse.quote_plus(password)\n",
        "\n",
        "# Build the connection string with the escaped password\n",
        "connection_string = f\"mongodb+srv://{username}:{escaped_password}@cluster0.8rqcf.mongodb.net/project?retryWrites=true&w=majority\"\n",
        "# Removing ssl=true&ssl_cert_reqs=ssl.CERT_NONE and adding tlsCAFile=certifi.where()\n",
        "# Updated connection string to use the system's trusted CA certificates\n",
        "\n",
        "\n",
        "# Set the TLS CA file to the certifi bundle\n",
        "# Adding tlsAllowInvalidCertificates=True to bypass SSL verification\n",
        "client = MongoClient(connection_string, tlsCAFile=certifi.where(), tlsAllowInvalidCertificates=True)\n",
        "\n",
        "db = client['twitter_data']\n",
        "collection = db['tesla_tweets']\n",
        "\n",
        "# Fetch all documents from the collection\n",
        "cursor = collection.find()  # This gets all documents, you can add filters if needed\n",
        "# Convert MongoDB documents to a pandas DataFrame\n",
        "data = pd.DataFrame(list(cursor))\n",
        "\n",
        "# Display the first few rows of the data\n",
        "print(data.head())\n",
        "# Data cleaning\n",
        "# Drop unnecessary columns\n",
        "dropped_columns = ['geo', 'place', 'thumbnail', 'quote_url', 'search', 'near', 'geo', 'user_rt_id', 'user_rt', 'retweet_id', 'retweet_date']\n",
        "tesla_data = data.drop(columns=dropped_columns, errors='ignore')\n",
        "\n",
        "# Drop duplicates and missing values\n",
        "# Convert list columns to tuples before dropping duplicates\n",
        "for column in tesla_data.columns:\n",
        "    if tesla_data[column].apply(lambda x: isinstance(x, list)).any():\n",
        "        tesla_data[column] = tesla_data[column].apply(tuple)\n",
        "\n",
        "tesla_data = tesla_data.drop_duplicates()\n",
        "tesla_data = tesla_data.dropna(subset=['tweet'])\n",
        "\n",
        "# Clean tweet text (remove URLs, mentions, hashtags etc.)\n",
        "def clean_tweet(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r\"@\\w+|#\\w+\", '', text)  # Remove mentions and hashtags\n",
        "    text = re.sub(r\"[^A-Za-z\\s]\", '', text)  # Remove special characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text.strip()\n",
        "\n",
        "tesla_data['cleaned_tweet'] = tesla_data['tweet'].apply(clean_tweet)\n",
        "\n",
        "# Reset index\n",
        "tesla_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Save cleaned data\n",
        "tesla_data.to_csv(\"cleaned_tesla_tweets.csv\", index=False)\n",
        "\n",
        "\n",
        "# Insert data\n",
        "records = tesla_data.to_dict('records')\n",
        "collection.delete_many({})  # Clear existing data\n",
        "collection.insert_many(records)\n",
        "\n",
        "print(\"Data inserted successfully into MongoDB!\")\n",
        "\n",
        "# Sentiment analysis\n",
        "def get_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    if analysis.sentiment.polarity > 0:\n",
        "        return \"Positive\"\n",
        "    elif analysis.sentiment.polarity < 0:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "tesla_data['sentiment'] = tesla_data['cleaned_tweet'].apply(get_sentiment)\n",
        "\n",
        "# Update MongoDB with sentiment\n",
        "for _, row in tesla_data.iterrows():\n",
        "    collection.update_one({'_id': row['_id']}, {'$set': {'sentiment': row['sentiment']}})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNysg5-Q--K1",
        "outputId": "03819869-6dff-4cef-e368-f1e995be7200"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.11/dist-packages (4.10.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (2024.12.14)\n",
            "                        _id                      id      conversation_id  \\\n",
            "0  678cd3f3dd4e86ee3a8d64b1  0  1546541426317590528  1545826164564000768   \n",
            "1  678cd3f3dd4e86ee3a8d64b2  1  1546541415857102850  1545826164564000768   \n",
            "2  678cd3f3dd4e86ee3a8d64b3  2  1546541411897581568  1546541411897581568   \n",
            "3  678cd3f3dd4e86ee3a8d64b4  3  1546541379110805508  1546340000500813824   \n",
            "4  678cd3f3dd4e86ee3a8d64b5  4  1546541363470028800  1546541363470028800   \n",
            "\n",
            "     created_at                 date timezone  \\\n",
            "0  1.657559e+12  2022-07-11 17:06:24    +0000   \n",
            "1  1.657559e+12  2022-07-11 17:06:21    +0000   \n",
            "2  1.657559e+12  2022-07-11 17:06:20    +0000   \n",
            "3  1.657559e+12  2022-07-11 17:06:12    +0000   \n",
            "4  1.657559e+12  2022-07-11 17:06:09    +0000   \n",
            "\n",
            "                                               tweet language  \\\n",
            "0  @GailAlfarATX @elonmusk @Tesla @teslacn @Tesla...       en   \n",
            "1  @elonmusk @GailAlfarATX @Tesla @teslacn @Tesla...       en   \n",
            "2  @elonmusk #Think about buying a country , #Mex...       en   \n",
            "3  @get_innocuous Actual receipts, and yet you ha...       en   \n",
            "4  Tesla wall battery for the save! Power went ou...       en   \n",
            "\n",
            "                                            hashtags  ... urls  photos  video  \\\n",
            "0                                                 []  ...   []      []      0   \n",
            "1                                                 []  ...   []      []      0   \n",
            "2  [[, ', t, h, i, n, k, ', ,,  , ', m, e, x, i, ...  ...   []      []      0   \n",
            "3                                                 []  ...   []      []      0   \n",
            "4                                                 []  ...   []      []      0   \n",
            "\n",
            "  retweet nlikes  nreplies  nretweets  \\\n",
            "0   False      0         0          0   \n",
            "1   False      0         0          0   \n",
            "2   False      0         0          0   \n",
            "3   False      0         0          0   \n",
            "4   False      0         0          0   \n",
            "\n",
            "                                            reply_to  \\\n",
            "0  [[, {, ', s, c, r, e, e, n, _, n, a, m, e, ', ...   \n",
            "1  [[, {, ', s, c, r, e, e, n, _, n, a, m, e, ', ...   \n",
            "2                                                 []   \n",
            "3  [[, {, ', s, c, r, e, e, n, _, n, a, m, e, ', ...   \n",
            "4                                                 []   \n",
            "\n",
            "                                       cleaned_tweet sentiment  \n",
            "0  i have six  of them still live at home being h...  Negative  \n",
            "1  then go for your dozen kids you are just missi...  Positive  \n",
            "2  about buying a country    you could turn it in...  Positive  \n",
            "3  actual receipts and yet you havent asked anyon...  Positive  \n",
            "4  tesla wall battery for the save power went out...   Neutral  \n",
            "\n",
            "[5 rows x 28 columns]\n",
            "Data inserted successfully into MongoDB!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tesla_data[['nlikes','tweet','cleaned_tweet','sentiment']])\n",
        "print(max(tesla_data['nlikes']))\n",
        "new_tesla_data = tesla_data[['nlikes','cleaned_tweet','sentiment']]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "dH0dJ4GIvI4N",
        "outputId": "b19439cf-7996-491a-dafc-d4bf987a8177"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tesla_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-add58b665260>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesla_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nlikes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cleaned_tweet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesla_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nlikes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew_tesla_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtesla_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nlikes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cleaned_tweet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tesla_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "# 1. Create TF-IDF vectors from cleaned tweets (using 'new_tesla_data')\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(new_tesla_data['cleaned_tweet'])\n",
        "\n",
        "# 2. Perform K-Means clustering (using 'new_tesla_data')\n",
        "num_clusters = 3  # For positive, neutral, and negative\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "new_tesla_data['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "# 3. Analyze clusters and assign sentiment labels\n",
        "cluster_mapping = {\n",
        "    0: 'Positive',\n",
        "    1: 'Neutral',\n",
        "    2: 'Negative'\n",
        "}\n",
        "new_tesla_data['predicted_sentiment'] = new_tesla_data['cluster'].map(cluster_mapping)\n",
        "\n",
        "# 4. Add 'predicted_sentiment' to 'data' DataFrame\n",
        "data = pd.merge(data, new_tesla_data[['cleaned_tweet', 'predicted_sentiment']], on='cleaned_tweet', how='left')\n",
        "\n",
        "# 5. Calculate correlation between 'nlikes' and sentiment (using 'data')\n",
        "correlation = data.groupby('predicted_sentiment')['nlikes'].mean()\n",
        "\n",
        "# 6. Display results\n",
        "print(\"Correlation between Likes and Sentiment:\")\n",
        "print(correlation)\n",
        "# Plot sentiment distribution\n",
        "sentiment_counts = data['sentiment'].value_counts()\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
        "plt.title(\"Sentiment Distribution\")\n",
        "plt.show()\n",
        "'''# Clustering\n",
        "assembler = VectorAssembler(inputCols=[\"nlikes\", \"nreplies\", \"nretweets\"], outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# K-Means clustering\n",
        "kmeans = KMeans(k=3, seed=1)  # Choose k clusters\n",
        "model = kmeans.fit(df)\n",
        "clusters = model.transform(df)\n",
        "\n",
        "# Display clusters\n",
        "clusters.select(\"tweet\", \"prediction\").show()\n",
        "\n",
        "\n",
        "# Plot sentiment distribution\n",
        "sentiment_counts = data['sentiment'].value_counts()\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
        "plt.title(\"Sentiment Distribution\")\n",
        "plt.show()'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "H3ylhdwtSgum",
        "outputId": "12969b06-df36-477d-a73a-e91724250620"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'new_tesla_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f912495ed959>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 1. Create TF-IDF vectors from cleaned tweets (using 'new_tesla_data')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtfidf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tesla_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 2. Perform K-Means clustering (using 'new_tesla_data')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'new_tesla_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4nfcSE9ntecK"
      },
      "execution_count": 59,
      "outputs": []
    }
  ]
}